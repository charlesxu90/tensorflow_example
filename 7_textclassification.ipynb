{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\") # column \n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\") # column?\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(\"./data/rt-polaritydata/rt-polarity.pos\", \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(\"./data/rt-polaritydata/rt-polarity.neg\", \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels()\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocabulary Size: 18765\n",
      "Train/Dev split: 1000/100\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "#x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "#y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]\n",
    "\n",
    "x_train, x_dev = x_shuffled[:1000], x_shuffled[-100:]\n",
    "y_train, y_dev = y_shuffled[:1000], y_shuffled[-100:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/charles/Documents/tensorflow/tensorflow/examples/udacity/runs/1462444065\n",
      "\n",
      "2016-05-05T18:27:47.143434: step 1, loss 3.35732, acc 0.375\n",
      "2016-05-05T18:27:47.446110: step 2, loss 2.7295, acc 0.359375\n",
      "2016-05-05T18:27:47.695666: step 3, loss 1.86194, acc 0.546875\n",
      "2016-05-05T18:27:47.951870: step 4, loss 2.0896, acc 0.515625\n",
      "2016-05-05T18:27:48.227795: step 5, loss 1.70861, acc 0.65625\n",
      "2016-05-05T18:27:48.464229: step 6, loss 1.47577, acc 0.625\n",
      "2016-05-05T18:27:48.701651: step 7, loss 1.64839, acc 0.578125\n",
      "2016-05-05T18:27:48.941787: step 8, loss 2.25375, acc 0.515625\n",
      "2016-05-05T18:27:49.180291: step 9, loss 1.69714, acc 0.484375\n",
      "2016-05-05T18:27:49.519628: step 10, loss 1.97281, acc 0.515625\n",
      "2016-05-05T18:27:49.823398: step 11, loss 2.71366, acc 0.3125\n",
      "2016-05-05T18:27:50.129496: step 12, loss 1.79837, acc 0.546875\n",
      "2016-05-05T18:27:50.479671: step 13, loss 1.52558, acc 0.5625\n",
      "2016-05-05T18:27:50.790870: step 14, loss 1.68052, acc 0.546875\n",
      "2016-05-05T18:27:51.078719: step 15, loss 1.99101, acc 0.4375\n",
      "2016-05-05T18:27:51.293949: step 16, loss 1.97629, acc 0.475\n",
      "2016-05-05T18:27:51.607595: step 17, loss 1.47586, acc 0.5625\n",
      "2016-05-05T18:27:51.890838: step 18, loss 1.35126, acc 0.578125\n",
      "2016-05-05T18:27:52.152463: step 19, loss 1.10988, acc 0.671875\n",
      "2016-05-05T18:27:52.387770: step 20, loss 1.35644, acc 0.625\n",
      "2016-05-05T18:27:52.663123: step 21, loss 1.41651, acc 0.625\n",
      "2016-05-05T18:27:52.911311: step 22, loss 1.14726, acc 0.5625\n",
      "2016-05-05T18:27:53.156026: step 23, loss 1.64097, acc 0.578125\n",
      "2016-05-05T18:27:53.409402: step 24, loss 1.54115, acc 0.578125\n",
      "2016-05-05T18:27:53.658584: step 25, loss 2.02571, acc 0.5\n",
      "2016-05-05T18:27:53.920063: step 26, loss 1.81098, acc 0.5625\n",
      "2016-05-05T18:27:54.177808: step 27, loss 1.44808, acc 0.625\n",
      "2016-05-05T18:27:54.431510: step 28, loss 1.26944, acc 0.5625\n",
      "2016-05-05T18:27:54.705048: step 29, loss 1.26591, acc 0.578125\n",
      "2016-05-05T18:27:55.028829: step 30, loss 1.12947, acc 0.640625\n",
      "2016-05-05T18:27:55.283626: step 31, loss 1.34401, acc 0.53125\n",
      "2016-05-05T18:27:55.476553: step 32, loss 1.10821, acc 0.575\n",
      "2016-05-05T18:27:55.720934: step 33, loss 1.5778, acc 0.578125\n",
      "2016-05-05T18:27:55.961152: step 34, loss 1.11985, acc 0.59375\n",
      "2016-05-05T18:27:56.201169: step 35, loss 1.19761, acc 0.6875\n",
      "2016-05-05T18:27:56.443575: step 36, loss 1.45415, acc 0.65625\n",
      "2016-05-05T18:27:56.683323: step 37, loss 0.966688, acc 0.6875\n",
      "2016-05-05T18:27:56.926235: step 38, loss 1.19476, acc 0.671875\n",
      "2016-05-05T18:27:57.168936: step 39, loss 0.681428, acc 0.734375\n",
      "2016-05-05T18:27:57.409557: step 40, loss 0.681814, acc 0.796875\n",
      "2016-05-05T18:27:57.650775: step 41, loss 0.665355, acc 0.71875\n",
      "2016-05-05T18:27:57.897049: step 42, loss 1.07725, acc 0.6875\n",
      "2016-05-05T18:27:58.144705: step 43, loss 1.03193, acc 0.65625\n",
      "2016-05-05T18:27:58.391634: step 44, loss 1.06794, acc 0.65625\n",
      "2016-05-05T18:27:58.641708: step 45, loss 1.63538, acc 0.546875\n",
      "2016-05-05T18:27:58.879929: step 46, loss 1.19279, acc 0.640625\n",
      "2016-05-05T18:27:59.118837: step 47, loss 0.906832, acc 0.6875\n",
      "2016-05-05T18:27:59.309865: step 48, loss 1.14115, acc 0.625\n",
      "2016-05-05T18:27:59.553739: step 49, loss 0.761947, acc 0.734375\n",
      "2016-05-05T18:27:59.798162: step 50, loss 1.25757, acc 0.671875\n",
      "2016-05-05T18:28:00.039068: step 51, loss 0.906024, acc 0.734375\n",
      "2016-05-05T18:28:00.284864: step 52, loss 0.74611, acc 0.6875\n",
      "2016-05-05T18:28:00.530710: step 53, loss 0.760011, acc 0.78125\n",
      "2016-05-05T18:28:00.775572: step 54, loss 1.26018, acc 0.640625\n",
      "2016-05-05T18:28:01.024245: step 55, loss 0.683319, acc 0.78125\n",
      "2016-05-05T18:28:01.270529: step 56, loss 1.53456, acc 0.609375\n",
      "2016-05-05T18:28:01.518199: step 57, loss 0.760093, acc 0.75\n",
      "2016-05-05T18:28:01.760699: step 58, loss 1.25541, acc 0.640625\n",
      "2016-05-05T18:28:02.076631: step 59, loss 0.68898, acc 0.734375\n",
      "2016-05-05T18:28:02.379282: step 60, loss 1.2781, acc 0.609375\n",
      "2016-05-05T18:28:02.655643: step 61, loss 1.07153, acc 0.625\n",
      "2016-05-05T18:28:02.924957: step 62, loss 0.909857, acc 0.6875\n",
      "2016-05-05T18:28:03.236306: step 63, loss 0.97171, acc 0.65625\n",
      "2016-05-05T18:28:03.446758: step 64, loss 1.1566, acc 0.675\n",
      "2016-05-05T18:28:03.737271: step 65, loss 1.03389, acc 0.640625\n",
      "2016-05-05T18:28:04.030591: step 66, loss 0.408392, acc 0.78125\n",
      "2016-05-05T18:28:04.298201: step 67, loss 0.969065, acc 0.734375\n",
      "2016-05-05T18:28:04.592628: step 68, loss 0.836511, acc 0.78125\n",
      "2016-05-05T18:28:04.843464: step 69, loss 0.961887, acc 0.671875\n",
      "2016-05-05T18:28:05.189833: step 70, loss 0.926416, acc 0.65625\n",
      "2016-05-05T18:28:05.474224: step 71, loss 0.896189, acc 0.71875\n",
      "2016-05-05T18:28:05.729129: step 72, loss 0.788883, acc 0.65625\n",
      "2016-05-05T18:28:05.998013: step 73, loss 0.870317, acc 0.65625\n",
      "2016-05-05T18:28:06.251608: step 74, loss 1.16159, acc 0.671875\n",
      "2016-05-05T18:28:06.502176: step 75, loss 0.594645, acc 0.765625\n",
      "2016-05-05T18:28:06.748958: step 76, loss 0.651706, acc 0.796875\n",
      "2016-05-05T18:28:06.998715: step 77, loss 0.708945, acc 0.78125\n",
      "2016-05-05T18:28:07.252433: step 78, loss 0.758456, acc 0.78125\n",
      "2016-05-05T18:28:07.588993: step 79, loss 1.13324, acc 0.625\n",
      "2016-05-05T18:28:07.836326: step 80, loss 1.14195, acc 0.725\n",
      "2016-05-05T18:28:08.112953: step 81, loss 0.748935, acc 0.71875\n",
      "2016-05-05T18:28:08.367446: step 82, loss 0.745558, acc 0.78125\n",
      "2016-05-05T18:28:08.617834: step 83, loss 0.273817, acc 0.90625\n",
      "2016-05-05T18:28:08.863578: step 84, loss 0.69608, acc 0.84375\n",
      "2016-05-05T18:28:09.116310: step 85, loss 1.02075, acc 0.703125\n",
      "2016-05-05T18:28:09.364804: step 86, loss 0.785195, acc 0.78125\n",
      "2016-05-05T18:28:09.613213: step 87, loss 0.720519, acc 0.734375\n",
      "2016-05-05T18:28:09.865310: step 88, loss 0.848739, acc 0.6875\n",
      "2016-05-05T18:28:10.112026: step 89, loss 0.443988, acc 0.796875\n",
      "2016-05-05T18:28:10.363497: step 90, loss 0.853308, acc 0.703125\n",
      "2016-05-05T18:28:10.612256: step 91, loss 0.777546, acc 0.734375\n",
      "2016-05-05T18:28:10.862664: step 92, loss 0.552562, acc 0.796875\n",
      "2016-05-05T18:28:11.109090: step 93, loss 0.945521, acc 0.71875\n",
      "2016-05-05T18:28:11.354996: step 94, loss 0.801422, acc 0.78125\n",
      "2016-05-05T18:28:11.705311: step 95, loss 1.19701, acc 0.640625\n",
      "2016-05-05T18:28:11.928158: step 96, loss 0.555084, acc 0.85\n",
      "2016-05-05T18:28:12.211752: step 97, loss 0.896391, acc 0.71875\n",
      "2016-05-05T18:28:12.518858: step 98, loss 0.895419, acc 0.671875\n",
      "2016-05-05T18:28:12.818708: step 99, loss 0.648735, acc 0.859375\n",
      "2016-05-05T18:28:13.112876: step 100, loss 0.82776, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2016-05-05T18:28:13.226201: step 100, loss 0.779434, acc 0.59\n",
      "\n",
      "Saved model checkpoint to /Users/charles/Documents/tensorflow/tensorflow/examples/udacity/runs/1462444065/checkpoints/model-100\n",
      "\n",
      "2016-05-05T18:28:14.159751: step 101, loss 0.60649, acc 0.8125\n",
      "2016-05-05T18:28:14.448413: step 102, loss 0.375479, acc 0.859375\n",
      "2016-05-05T18:28:14.729337: step 103, loss 0.753116, acc 0.78125\n",
      "2016-05-05T18:28:15.049767: step 104, loss 0.738218, acc 0.65625\n",
      "2016-05-05T18:28:15.363109: step 105, loss 0.317318, acc 0.890625\n",
      "2016-05-05T18:28:15.654168: step 106, loss 0.738293, acc 0.75\n",
      "2016-05-05T18:28:15.950771: step 107, loss 0.50226, acc 0.78125\n",
      "2016-05-05T18:28:16.232487: step 108, loss 0.91248, acc 0.65625\n",
      "2016-05-05T18:28:16.487201: step 109, loss 0.394123, acc 0.78125\n",
      "2016-05-05T18:28:16.746865: step 110, loss 0.78631, acc 0.703125\n",
      "2016-05-05T18:28:17.013373: step 111, loss 0.722117, acc 0.671875\n",
      "2016-05-05T18:28:17.219065: step 112, loss 0.743758, acc 0.775\n",
      "2016-05-05T18:28:17.476845: step 113, loss 0.535733, acc 0.78125\n",
      "2016-05-05T18:28:17.732219: step 114, loss 0.466913, acc 0.8125\n",
      "2016-05-05T18:28:18.005086: step 115, loss 0.625544, acc 0.796875\n",
      "2016-05-05T18:28:18.245627: step 116, loss 0.560611, acc 0.796875\n",
      "2016-05-05T18:28:18.489183: step 117, loss 0.393699, acc 0.859375\n",
      "2016-05-05T18:28:18.749631: step 118, loss 0.413912, acc 0.828125\n",
      "2016-05-05T18:28:19.034063: step 119, loss 0.655915, acc 0.765625\n",
      "2016-05-05T18:28:19.362270: step 120, loss 0.349661, acc 0.859375\n",
      "2016-05-05T18:28:19.634556: step 121, loss 0.786038, acc 0.75\n",
      "2016-05-05T18:28:19.900558: step 122, loss 0.322875, acc 0.875\n",
      "2016-05-05T18:28:20.175636: step 123, loss 0.558962, acc 0.796875\n",
      "2016-05-05T18:28:20.445762: step 124, loss 0.337974, acc 0.875\n",
      "2016-05-05T18:28:20.724058: step 125, loss 0.290274, acc 0.828125\n",
      "2016-05-05T18:28:20.981248: step 126, loss 0.449985, acc 0.8125\n",
      "2016-05-05T18:28:21.221602: step 127, loss 0.704368, acc 0.765625\n",
      "2016-05-05T18:28:21.413913: step 128, loss 0.571766, acc 0.775\n",
      "2016-05-05T18:28:21.672787: step 129, loss 0.407317, acc 0.78125\n",
      "2016-05-05T18:28:21.929334: step 130, loss 0.415783, acc 0.84375\n",
      "2016-05-05T18:28:22.183215: step 131, loss 0.425909, acc 0.796875\n",
      "2016-05-05T18:28:22.435538: step 132, loss 0.486234, acc 0.796875\n",
      "2016-05-05T18:28:22.683682: step 133, loss 0.490649, acc 0.859375\n",
      "2016-05-05T18:28:22.932876: step 134, loss 0.289562, acc 0.875\n",
      "2016-05-05T18:28:23.194792: step 135, loss 0.511004, acc 0.796875\n",
      "2016-05-05T18:28:23.465485: step 136, loss 0.702927, acc 0.78125\n",
      "2016-05-05T18:28:23.716604: step 137, loss 0.23517, acc 0.921875\n",
      "2016-05-05T18:28:23.969717: step 138, loss 0.499689, acc 0.8125\n",
      "2016-05-05T18:28:24.261061: step 139, loss 0.486975, acc 0.828125\n",
      "2016-05-05T18:28:24.528700: step 140, loss 0.509642, acc 0.875\n",
      "2016-05-05T18:28:24.783008: step 141, loss 0.617984, acc 0.71875\n",
      "2016-05-05T18:28:25.042946: step 142, loss 0.285999, acc 0.859375\n",
      "2016-05-05T18:28:25.305166: step 143, loss 0.504826, acc 0.765625\n",
      "2016-05-05T18:28:25.499814: step 144, loss 0.371251, acc 0.825\n",
      "2016-05-05T18:28:25.758013: step 145, loss 0.471415, acc 0.828125\n",
      "2016-05-05T18:28:26.011069: step 146, loss 0.467642, acc 0.828125\n",
      "2016-05-05T18:28:26.260170: step 147, loss 0.366468, acc 0.8125\n",
      "2016-05-05T18:28:26.508371: step 148, loss 0.456972, acc 0.8125\n",
      "2016-05-05T18:28:26.754087: step 149, loss 0.243298, acc 0.9375\n",
      "2016-05-05T18:28:26.992878: step 150, loss 0.360499, acc 0.84375\n",
      "2016-05-05T18:28:27.240607: step 151, loss 0.320065, acc 0.859375\n",
      "2016-05-05T18:28:27.505747: step 152, loss 0.477541, acc 0.8125\n",
      "2016-05-05T18:28:27.788623: step 153, loss 0.585346, acc 0.8125\n",
      "2016-05-05T18:28:28.049907: step 154, loss 0.408266, acc 0.78125\n",
      "2016-05-05T18:28:28.288033: step 155, loss 0.415844, acc 0.84375\n",
      "2016-05-05T18:28:28.538023: step 156, loss 0.519472, acc 0.796875\n",
      "2016-05-05T18:28:28.789347: step 157, loss 0.59729, acc 0.84375\n",
      "2016-05-05T18:28:29.035048: step 158, loss 0.67045, acc 0.796875\n",
      "2016-05-05T18:28:29.280582: step 159, loss 0.384462, acc 0.796875\n",
      "2016-05-05T18:28:29.478953: step 160, loss 0.221931, acc 0.9\n",
      "2016-05-05T18:28:29.726380: step 161, loss 0.366503, acc 0.84375\n",
      "2016-05-05T18:28:29.980458: step 162, loss 0.288826, acc 0.859375\n",
      "2016-05-05T18:28:30.223523: step 163, loss 0.411805, acc 0.828125\n",
      "2016-05-05T18:28:30.489128: step 164, loss 0.373809, acc 0.8125\n",
      "2016-05-05T18:28:30.742032: step 165, loss 0.239505, acc 0.921875\n",
      "2016-05-05T18:28:30.988301: step 166, loss 0.36791, acc 0.859375\n",
      "2016-05-05T18:28:31.241487: step 167, loss 0.240625, acc 0.9375\n",
      "2016-05-05T18:28:31.485256: step 168, loss 0.406295, acc 0.875\n",
      "2016-05-05T18:28:31.721582: step 169, loss 0.188742, acc 0.921875\n",
      "2016-05-05T18:28:31.972377: step 170, loss 0.484484, acc 0.890625\n",
      "2016-05-05T18:28:32.231408: step 171, loss 0.174063, acc 0.921875\n",
      "2016-05-05T18:28:32.465716: step 172, loss 0.385774, acc 0.859375\n",
      "2016-05-05T18:28:32.714824: step 173, loss 0.485775, acc 0.78125\n",
      "2016-05-05T18:28:32.974865: step 174, loss 0.226444, acc 0.90625\n",
      "2016-05-05T18:28:33.237519: step 175, loss 0.373153, acc 0.84375\n",
      "2016-05-05T18:28:33.428407: step 176, loss 0.634083, acc 0.775\n",
      "2016-05-05T18:28:33.672497: step 177, loss 0.316568, acc 0.875\n",
      "2016-05-05T18:28:33.924046: step 178, loss 0.329421, acc 0.890625\n",
      "2016-05-05T18:28:34.186734: step 179, loss 0.351708, acc 0.90625\n",
      "2016-05-05T18:28:34.419492: step 180, loss 0.305159, acc 0.875\n",
      "2016-05-05T18:28:34.697073: step 181, loss 0.382507, acc 0.859375\n",
      "2016-05-05T18:28:34.964967: step 182, loss 0.236688, acc 0.90625\n",
      "2016-05-05T18:28:35.218871: step 183, loss 0.147668, acc 0.953125\n",
      "2016-05-05T18:28:35.468870: step 184, loss 0.335041, acc 0.859375\n",
      "2016-05-05T18:28:35.720962: step 185, loss 0.450967, acc 0.84375\n",
      "2016-05-05T18:28:35.974591: step 186, loss 0.30113, acc 0.875\n",
      "2016-05-05T18:28:36.227187: step 187, loss 0.27427, acc 0.859375\n",
      "2016-05-05T18:28:36.491682: step 188, loss 0.2602, acc 0.90625\n",
      "2016-05-05T18:28:36.751713: step 189, loss 0.350522, acc 0.875\n",
      "2016-05-05T18:28:37.000108: step 190, loss 0.348348, acc 0.875\n",
      "2016-05-05T18:28:37.254557: step 191, loss 0.27129, acc 0.890625\n",
      "2016-05-05T18:28:37.466421: step 192, loss 0.247228, acc 0.9\n",
      "2016-05-05T18:28:37.731937: step 193, loss 0.345046, acc 0.828125\n",
      "2016-05-05T18:28:37.990522: step 194, loss 0.246143, acc 0.90625\n",
      "2016-05-05T18:28:38.247218: step 195, loss 0.42092, acc 0.859375\n",
      "2016-05-05T18:28:38.505523: step 196, loss 0.130993, acc 0.9375\n",
      "2016-05-05T18:28:38.748732: step 197, loss 0.377032, acc 0.828125\n",
      "2016-05-05T18:28:38.999355: step 198, loss 0.269406, acc 0.890625\n",
      "2016-05-05T18:28:39.248223: step 199, loss 0.264726, acc 0.90625\n",
      "2016-05-05T18:28:39.527926: step 200, loss 0.221905, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2016-05-05T18:28:39.623659: step 200, loss 0.740159, acc 0.56\n",
      "\n",
      "Saved model checkpoint to /Users/charles/Documents/tensorflow/tensorflow/examples/udacity/runs/1462444065/checkpoints/model-200\n",
      "\n",
      "2016-05-05T18:28:40.288187: step 201, loss 0.250868, acc 0.90625\n",
      "2016-05-05T18:28:40.535578: step 202, loss 0.216918, acc 0.890625\n",
      "2016-05-05T18:28:40.823333: step 203, loss 0.218716, acc 0.890625\n",
      "2016-05-05T18:28:41.084762: step 204, loss 0.199201, acc 0.921875\n",
      "2016-05-05T18:28:41.338774: step 205, loss 0.248598, acc 0.90625\n",
      "2016-05-05T18:28:41.590868: step 206, loss 0.520456, acc 0.84375\n",
      "2016-05-05T18:28:41.854344: step 207, loss 0.177116, acc 0.90625\n",
      "2016-05-05T18:28:42.046316: step 208, loss 0.271433, acc 0.9\n",
      "2016-05-05T18:28:42.330957: step 209, loss 0.0875576, acc 0.984375\n",
      "2016-05-05T18:28:42.589876: step 210, loss 0.318435, acc 0.84375\n",
      "2016-05-05T18:28:42.852891: step 211, loss 0.266956, acc 0.859375\n",
      "2016-05-05T18:28:43.106020: step 212, loss 0.240259, acc 0.9375\n",
      "2016-05-05T18:28:43.358431: step 213, loss 0.269227, acc 0.875\n",
      "2016-05-05T18:28:43.601354: step 214, loss 0.0704499, acc 0.984375\n",
      "2016-05-05T18:28:43.867836: step 215, loss 0.18762, acc 0.9375\n",
      "2016-05-05T18:28:44.132246: step 216, loss 0.175881, acc 0.9375\n",
      "2016-05-05T18:28:44.385133: step 217, loss 0.305273, acc 0.859375\n",
      "2016-05-05T18:28:44.655070: step 218, loss 0.407077, acc 0.875\n",
      "2016-05-05T18:28:44.902721: step 219, loss 0.274483, acc 0.890625\n",
      "2016-05-05T18:28:45.159071: step 220, loss 0.316603, acc 0.84375\n",
      "2016-05-05T18:28:45.418253: step 221, loss 0.368286, acc 0.828125\n",
      "2016-05-05T18:28:45.671270: step 222, loss 0.415708, acc 0.859375\n",
      "2016-05-05T18:28:45.917968: step 223, loss 0.368505, acc 0.875\n",
      "2016-05-05T18:28:46.108818: step 224, loss 0.150353, acc 0.95\n",
      "2016-05-05T18:28:46.369404: step 225, loss 0.382231, acc 0.859375\n",
      "2016-05-05T18:28:46.632422: step 226, loss 0.235355, acc 0.890625\n",
      "2016-05-05T18:28:46.884765: step 227, loss 0.180196, acc 0.9375\n",
      "2016-05-05T18:28:47.127055: step 228, loss 0.316779, acc 0.890625\n",
      "2016-05-05T18:28:47.378974: step 229, loss 0.334656, acc 0.859375\n",
      "2016-05-05T18:28:47.630181: step 230, loss 0.326551, acc 0.875\n",
      "2016-05-05T18:28:47.877104: step 231, loss 0.322791, acc 0.828125\n",
      "2016-05-05T18:28:48.127215: step 232, loss 0.231214, acc 0.890625\n",
      "2016-05-05T18:28:48.396269: step 233, loss 0.186485, acc 0.90625\n",
      "2016-05-05T18:28:48.646464: step 234, loss 0.178502, acc 0.921875\n",
      "2016-05-05T18:28:48.903941: step 235, loss 0.17111, acc 0.921875\n",
      "2016-05-05T18:28:49.162189: step 236, loss 0.338338, acc 0.8125\n",
      "2016-05-05T18:28:49.420357: step 237, loss 0.226042, acc 0.890625\n",
      "2016-05-05T18:28:49.692133: step 238, loss 0.48872, acc 0.828125\n",
      "2016-05-05T18:28:49.965965: step 239, loss 0.40004, acc 0.8125\n",
      "2016-05-05T18:28:50.145052: step 240, loss 0.156769, acc 0.95\n",
      "2016-05-05T18:28:50.398164: step 241, loss 0.520964, acc 0.796875\n",
      "2016-05-05T18:28:50.633457: step 242, loss 0.107019, acc 0.984375\n",
      "2016-05-05T18:28:50.871971: step 243, loss 0.174637, acc 0.953125\n",
      "2016-05-05T18:28:51.120220: step 244, loss 0.256539, acc 0.90625\n",
      "2016-05-05T18:28:51.378937: step 245, loss 0.366332, acc 0.84375\n",
      "2016-05-05T18:28:51.631052: step 246, loss 0.237767, acc 0.90625\n",
      "2016-05-05T18:28:51.876880: step 247, loss 0.182096, acc 0.921875\n",
      "2016-05-05T18:28:52.129659: step 248, loss 0.249872, acc 0.875\n",
      "2016-05-05T18:28:52.401348: step 249, loss 0.0680939, acc 0.984375\n",
      "2016-05-05T18:28:52.663136: step 250, loss 0.378014, acc 0.875\n",
      "2016-05-05T18:28:52.927635: step 251, loss 0.179117, acc 0.9375\n",
      "2016-05-05T18:28:53.181818: step 252, loss 0.18917, acc 0.875\n",
      "2016-05-05T18:28:53.425850: step 253, loss 0.166951, acc 0.9375\n",
      "2016-05-05T18:28:53.674065: step 254, loss 0.178618, acc 0.9375\n",
      "2016-05-05T18:28:53.904482: step 255, loss 0.195887, acc 0.9375\n",
      "2016-05-05T18:28:54.092495: step 256, loss 0.140377, acc 0.95\n",
      "2016-05-05T18:28:54.349088: step 257, loss 0.324087, acc 0.875\n",
      "2016-05-05T18:28:54.603516: step 258, loss 0.098442, acc 0.96875\n",
      "2016-05-05T18:28:54.867681: step 259, loss 0.0844774, acc 0.96875\n",
      "2016-05-05T18:28:55.117900: step 260, loss 0.204753, acc 0.953125\n",
      "2016-05-05T18:28:55.396201: step 261, loss 0.0750351, acc 0.984375\n",
      "2016-05-05T18:28:55.652202: step 262, loss 0.205665, acc 0.9375\n",
      "2016-05-05T18:28:55.900289: step 263, loss 0.235024, acc 0.921875\n",
      "2016-05-05T18:28:56.149622: step 264, loss 0.127876, acc 0.921875\n",
      "2016-05-05T18:28:56.401828: step 265, loss 0.44029, acc 0.84375\n",
      "2016-05-05T18:28:56.653351: step 266, loss 0.11081, acc 0.984375\n",
      "2016-05-05T18:28:56.899960: step 267, loss 0.162361, acc 0.9375\n",
      "2016-05-05T18:28:57.142049: step 268, loss 0.272134, acc 0.90625\n",
      "2016-05-05T18:28:57.400249: step 269, loss 0.234238, acc 0.921875\n",
      "2016-05-05T18:28:57.664793: step 270, loss 0.205648, acc 0.875\n",
      "2016-05-05T18:28:57.914660: step 271, loss 0.17014, acc 0.953125\n",
      "2016-05-05T18:28:58.096849: step 272, loss 0.0451818, acc 1\n",
      "2016-05-05T18:28:58.362844: step 273, loss 0.207845, acc 0.921875\n",
      "2016-05-05T18:28:58.616276: step 274, loss 0.0702018, acc 0.984375\n",
      "2016-05-05T18:28:58.870515: step 275, loss 0.0800743, acc 0.984375\n",
      "2016-05-05T18:28:59.118281: step 276, loss 0.168189, acc 0.953125\n",
      "2016-05-05T18:28:59.362078: step 277, loss 0.183867, acc 0.9375\n",
      "2016-05-05T18:28:59.624466: step 278, loss 0.106947, acc 0.9375\n",
      "2016-05-05T18:28:59.880440: step 279, loss 0.099364, acc 0.984375\n",
      "2016-05-05T18:29:00.130480: step 280, loss 0.153478, acc 0.9375\n",
      "2016-05-05T18:29:00.377468: step 281, loss 0.13616, acc 0.921875\n",
      "2016-05-05T18:29:00.625538: step 282, loss 0.303007, acc 0.90625\n",
      "2016-05-05T18:29:00.885656: step 283, loss 0.144438, acc 0.9375\n",
      "2016-05-05T18:29:01.140950: step 284, loss 0.293172, acc 0.875\n",
      "2016-05-05T18:29:01.385338: step 285, loss 0.210102, acc 0.9375\n",
      "2016-05-05T18:29:01.645007: step 286, loss 0.154931, acc 0.9375\n",
      "2016-05-05T18:29:01.900909: step 287, loss 0.225358, acc 0.921875\n",
      "2016-05-05T18:29:02.088264: step 288, loss 0.183299, acc 0.925\n",
      "2016-05-05T18:29:02.338347: step 289, loss 0.140782, acc 0.953125\n",
      "2016-05-05T18:29:02.592839: step 290, loss 0.105817, acc 0.96875\n",
      "2016-05-05T18:29:02.851257: step 291, loss 0.173059, acc 0.9375\n",
      "2016-05-05T18:29:03.089272: step 292, loss 0.204677, acc 0.9375\n",
      "2016-05-05T18:29:03.341356: step 293, loss 0.119176, acc 0.9375\n",
      "2016-05-05T18:29:03.575176: step 294, loss 0.155434, acc 0.921875\n",
      "2016-05-05T18:29:03.821342: step 295, loss 0.0815415, acc 0.96875\n",
      "2016-05-05T18:29:04.082633: step 296, loss 0.182911, acc 0.9375\n",
      "2016-05-05T18:29:04.348310: step 297, loss 0.114586, acc 0.96875\n",
      "2016-05-05T18:29:04.637382: step 298, loss 0.070913, acc 0.96875\n",
      "2016-05-05T18:29:04.900211: step 299, loss 0.10072, acc 0.9375\n",
      "2016-05-05T18:29:05.151804: step 300, loss 0.288606, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2016-05-05T18:29:05.250254: step 300, loss 0.746903, acc 0.59\n",
      "\n",
      "Saved model checkpoint to /Users/charles/Documents/tensorflow/tensorflow/examples/udacity/runs/1462444065/checkpoints/model-300\n",
      "\n",
      "2016-05-05T18:29:05.905874: step 301, loss 0.109648, acc 0.921875\n",
      "2016-05-05T18:29:06.156618: step 302, loss 0.17003, acc 0.96875\n",
      "2016-05-05T18:29:06.414487: step 303, loss 0.0901835, acc 0.953125\n",
      "2016-05-05T18:29:06.617863: step 304, loss 0.164022, acc 0.95\n",
      "2016-05-05T18:29:06.871632: step 305, loss 0.250351, acc 0.890625\n",
      "2016-05-05T18:29:07.120460: step 306, loss 0.164944, acc 0.921875\n",
      "2016-05-05T18:29:07.370184: step 307, loss 0.319543, acc 0.84375\n",
      "2016-05-05T18:29:07.622862: step 308, loss 0.205058, acc 0.9375\n",
      "2016-05-05T18:29:07.875778: step 309, loss 0.0986338, acc 0.953125\n",
      "2016-05-05T18:29:08.137541: step 310, loss 0.162806, acc 0.9375\n",
      "2016-05-05T18:29:08.381128: step 311, loss 0.092161, acc 0.953125\n",
      "2016-05-05T18:29:08.623306: step 312, loss 0.0814476, acc 0.96875\n",
      "2016-05-05T18:29:08.867324: step 313, loss 0.218065, acc 0.9375\n",
      "2016-05-05T18:29:09.107297: step 314, loss 0.240135, acc 0.875\n",
      "2016-05-05T18:29:09.359800: step 315, loss 0.140874, acc 0.96875\n",
      "2016-05-05T18:29:09.627703: step 316, loss 0.0911788, acc 0.96875\n",
      "2016-05-05T18:29:09.883932: step 317, loss 0.311098, acc 0.859375\n",
      "2016-05-05T18:29:10.126418: step 318, loss 0.0789117, acc 1\n",
      "2016-05-05T18:29:10.388504: step 319, loss 0.221254, acc 0.921875\n",
      "2016-05-05T18:29:10.563272: step 320, loss 0.0499768, acc 0.975\n",
      "2016-05-05T18:29:10.813682: step 321, loss 0.243875, acc 0.921875\n",
      "2016-05-05T18:29:11.076367: step 322, loss 0.0847342, acc 0.96875\n",
      "2016-05-05T18:29:11.339335: step 323, loss 0.0803758, acc 0.984375\n",
      "2016-05-05T18:29:11.603047: step 324, loss 0.111866, acc 0.953125\n",
      "2016-05-05T18:29:11.833744: step 325, loss 0.158511, acc 0.9375\n",
      "2016-05-05T18:29:12.079128: step 326, loss 0.189513, acc 0.921875\n",
      "2016-05-05T18:29:12.328624: step 327, loss 0.128003, acc 0.9375\n",
      "2016-05-05T18:29:12.579629: step 328, loss 0.0984823, acc 0.953125\n",
      "2016-05-05T18:29:12.828975: step 329, loss 0.0769306, acc 0.953125\n",
      "2016-05-05T18:29:13.092948: step 330, loss 0.157373, acc 0.921875\n",
      "2016-05-05T18:29:13.351719: step 331, loss 0.222617, acc 0.921875\n",
      "2016-05-05T18:29:13.617655: step 332, loss 0.193139, acc 0.90625\n",
      "2016-05-05T18:29:13.867181: step 333, loss 0.181163, acc 0.90625\n",
      "2016-05-05T18:29:14.123850: step 334, loss 0.206632, acc 0.90625\n",
      "2016-05-05T18:29:14.388143: step 335, loss 0.122877, acc 0.921875\n",
      "2016-05-05T18:29:14.598004: step 336, loss 0.188821, acc 0.95\n",
      "2016-05-05T18:29:14.844096: step 337, loss 0.144775, acc 0.984375\n",
      "2016-05-05T18:29:15.097739: step 338, loss 0.0806819, acc 0.953125\n",
      "2016-05-05T18:29:15.348648: step 339, loss 0.0917801, acc 0.96875\n",
      "2016-05-05T18:29:15.595748: step 340, loss 0.10724, acc 0.953125\n",
      "2016-05-05T18:29:15.839098: step 341, loss 0.270345, acc 0.921875\n",
      "2016-05-05T18:29:16.104398: step 342, loss 0.0883201, acc 0.953125\n",
      "2016-05-05T18:29:16.350077: step 343, loss 0.0345178, acc 1\n",
      "2016-05-05T18:29:16.614062: step 344, loss 0.118716, acc 0.953125\n",
      "2016-05-05T18:29:16.871927: step 345, loss 0.144775, acc 0.96875\n",
      "2016-05-05T18:29:17.105009: step 346, loss 0.161925, acc 0.890625\n",
      "2016-05-05T18:29:17.346458: step 347, loss 0.115848, acc 0.921875\n",
      "2016-05-05T18:29:17.591086: step 348, loss 0.182289, acc 0.953125\n",
      "2016-05-05T18:29:17.831177: step 349, loss 0.17344, acc 0.9375\n",
      "2016-05-05T18:29:18.079537: step 350, loss 0.090078, acc 0.953125\n",
      "2016-05-05T18:29:18.331785: step 351, loss 0.243254, acc 0.90625\n",
      "2016-05-05T18:29:18.518534: step 352, loss 0.0477547, acc 1\n",
      "2016-05-05T18:29:18.762055: step 353, loss 0.123639, acc 0.9375\n",
      "2016-05-05T18:29:19.010669: step 354, loss 0.0712535, acc 0.96875\n",
      "2016-05-05T18:29:19.260316: step 355, loss 0.0813545, acc 0.96875\n",
      "2016-05-05T18:29:19.520176: step 356, loss 0.200361, acc 0.96875\n",
      "2016-05-05T18:29:19.769750: step 357, loss 0.133832, acc 0.953125\n",
      "2016-05-05T18:29:20.026200: step 358, loss 0.157018, acc 0.9375\n",
      "2016-05-05T18:29:20.281776: step 359, loss 0.102222, acc 0.953125\n",
      "2016-05-05T18:29:20.529691: step 360, loss 0.129818, acc 0.96875\n",
      "2016-05-05T18:29:20.789423: step 361, loss 0.0685901, acc 0.96875\n",
      "2016-05-05T18:29:21.038716: step 362, loss 0.0435735, acc 1\n",
      "2016-05-05T18:29:21.287974: step 363, loss 0.0992823, acc 0.953125\n",
      "2016-05-05T18:29:21.548140: step 364, loss 0.206356, acc 0.90625\n",
      "2016-05-05T18:29:21.780477: step 365, loss 0.066505, acc 0.984375\n",
      "2016-05-05T18:29:22.024617: step 366, loss 0.0654617, acc 0.984375\n",
      "2016-05-05T18:29:22.280785: step 367, loss 0.135396, acc 0.9375\n",
      "2016-05-05T18:29:22.477214: step 368, loss 0.0522313, acc 0.975\n",
      "2016-05-05T18:29:22.736833: step 369, loss 0.130758, acc 0.96875\n",
      "2016-05-05T18:29:22.983115: step 370, loss 0.0637521, acc 0.984375\n",
      "2016-05-05T18:29:23.238961: step 371, loss 0.132162, acc 0.9375\n",
      "2016-05-05T18:29:23.502059: step 372, loss 0.159867, acc 0.921875\n",
      "2016-05-05T18:29:23.745203: step 373, loss 0.131651, acc 0.921875\n",
      "2016-05-05T18:29:23.991716: step 374, loss 0.231193, acc 0.90625\n",
      "2016-05-05T18:29:24.245068: step 375, loss 0.0922925, acc 0.96875\n",
      "2016-05-05T18:29:24.497388: step 376, loss 0.0425092, acc 1\n",
      "2016-05-05T18:29:24.748483: step 377, loss 0.173086, acc 0.953125\n",
      "2016-05-05T18:29:25.000128: step 378, loss 0.0426218, acc 0.984375\n",
      "2016-05-05T18:29:25.239929: step 379, loss 0.0599707, acc 0.96875\n",
      "2016-05-05T18:29:25.496086: step 380, loss 0.139696, acc 0.96875\n",
      "2016-05-05T18:29:25.752697: step 381, loss 0.0680752, acc 0.984375\n",
      "2016-05-05T18:29:25.989730: step 382, loss 0.174577, acc 0.9375\n",
      "2016-05-05T18:29:26.237628: step 383, loss 0.176158, acc 0.953125\n",
      "2016-05-05T18:29:26.418444: step 384, loss 0.184402, acc 0.925\n",
      "2016-05-05T18:29:26.678686: step 385, loss 0.103649, acc 0.984375\n",
      "2016-05-05T18:29:26.933181: step 386, loss 0.0988289, acc 0.96875\n",
      "2016-05-05T18:29:27.192614: step 387, loss 0.146772, acc 0.953125\n",
      "2016-05-05T18:29:27.434997: step 388, loss 0.110939, acc 0.96875\n",
      "2016-05-05T18:29:27.688999: step 389, loss 0.20801, acc 0.921875\n",
      "2016-05-05T18:29:27.944486: step 390, loss 0.172977, acc 0.9375\n",
      "2016-05-05T18:29:28.210419: step 391, loss 0.0708652, acc 0.984375\n",
      "2016-05-05T18:29:28.487014: step 392, loss 0.138361, acc 0.953125\n",
      "2016-05-05T18:29:28.740705: step 393, loss 0.0537065, acc 1\n",
      "2016-05-05T18:29:28.997510: step 394, loss 0.0753075, acc 0.96875\n",
      "2016-05-05T18:29:29.251364: step 395, loss 0.114348, acc 0.984375\n",
      "2016-05-05T18:29:29.503760: step 396, loss 0.326239, acc 0.90625\n",
      "2016-05-05T18:29:29.760660: step 397, loss 0.178714, acc 0.9375\n",
      "2016-05-05T18:29:30.012013: step 398, loss 0.147728, acc 0.9375\n",
      "2016-05-05T18:29:30.261750: step 399, loss 0.0796136, acc 0.96875\n",
      "2016-05-05T18:29:30.470718: step 400, loss 0.224843, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2016-05-05T18:29:30.572416: step 400, loss 0.822207, acc 0.62\n",
      "\n",
      "Saved model checkpoint to /Users/charles/Documents/tensorflow/tensorflow/examples/udacity/runs/1462444065/checkpoints/model-400\n",
      "\n",
      "2016-05-05T18:29:31.480519: step 401, loss 0.095347, acc 0.953125\n",
      "2016-05-05T18:29:31.721597: step 402, loss 0.152421, acc 0.921875\n",
      "2016-05-05T18:29:31.974347: step 403, loss 0.0593089, acc 0.96875\n",
      "2016-05-05T18:29:32.217962: step 404, loss 0.139044, acc 0.9375\n",
      "2016-05-05T18:29:32.460587: step 405, loss 0.062779, acc 0.96875\n",
      "2016-05-05T18:29:32.704488: step 406, loss 0.0807747, acc 0.953125\n",
      "2016-05-05T18:29:32.955798: step 407, loss 0.0429671, acc 1\n",
      "2016-05-05T18:29:33.208647: step 408, loss 0.118524, acc 0.953125\n",
      "2016-05-05T18:29:33.460853: step 409, loss 0.147558, acc 0.96875\n",
      "2016-05-05T18:29:33.713819: step 410, loss 0.0527678, acc 0.984375\n",
      "2016-05-05T18:29:33.963673: step 411, loss 0.0832564, acc 0.96875\n",
      "2016-05-05T18:29:34.214867: step 412, loss 0.0742757, acc 0.984375\n",
      "2016-05-05T18:29:34.469150: step 413, loss 0.165216, acc 0.921875\n",
      "2016-05-05T18:29:34.734502: step 414, loss 0.0217712, acc 1\n",
      "2016-05-05T18:29:34.984418: step 415, loss 0.0970076, acc 0.953125\n",
      "2016-05-05T18:29:35.168215: step 416, loss 0.0922686, acc 0.975\n",
      "2016-05-05T18:29:35.412454: step 417, loss 0.0292886, acc 1\n",
      "2016-05-05T18:29:35.662921: step 418, loss 0.107878, acc 0.96875\n",
      "2016-05-05T18:29:35.913752: step 419, loss 0.0785227, acc 0.96875\n",
      "2016-05-05T18:29:36.161321: step 420, loss 0.0923161, acc 0.96875\n",
      "2016-05-05T18:29:36.411534: step 421, loss 0.0213731, acc 1\n",
      "2016-05-05T18:29:36.648902: step 422, loss 0.100673, acc 0.984375\n",
      "2016-05-05T18:29:36.887151: step 423, loss 0.164809, acc 0.921875\n",
      "2016-05-05T18:29:37.139035: step 424, loss 0.0667768, acc 0.96875\n",
      "2016-05-05T18:29:37.410397: step 425, loss 0.0758863, acc 0.96875\n",
      "2016-05-05T18:29:37.677430: step 426, loss 0.0467511, acc 0.984375\n",
      "2016-05-05T18:29:37.922012: step 427, loss 0.0609776, acc 0.984375\n",
      "2016-05-05T18:29:38.149231: step 428, loss 0.0858396, acc 0.96875\n",
      "2016-05-05T18:29:38.391494: step 429, loss 0.117397, acc 0.953125\n",
      "2016-05-05T18:29:38.652908: step 430, loss 0.0647638, acc 0.984375\n",
      "2016-05-05T18:29:38.920715: step 431, loss 0.199485, acc 0.953125\n",
      "2016-05-05T18:29:39.112182: step 432, loss 0.112122, acc 0.925\n",
      "2016-05-05T18:29:39.354692: step 433, loss 0.132539, acc 0.953125\n",
      "2016-05-05T18:29:39.613947: step 434, loss 0.045562, acc 0.984375\n",
      "2016-05-05T18:29:39.866445: step 435, loss 0.111198, acc 0.96875\n",
      "2016-05-05T18:29:40.106404: step 436, loss 0.101129, acc 0.953125\n",
      "2016-05-05T18:29:40.352796: step 437, loss 0.042899, acc 0.984375\n",
      "2016-05-05T18:29:40.608092: step 438, loss 0.0815182, acc 0.953125\n",
      "2016-05-05T18:29:40.866395: step 439, loss 0.118335, acc 0.921875\n",
      "2016-05-05T18:29:41.097209: step 440, loss 0.091388, acc 0.96875\n",
      "2016-05-05T18:29:41.337037: step 441, loss 0.0776031, acc 0.96875\n",
      "2016-05-05T18:29:41.567580: step 442, loss 0.0898949, acc 0.984375\n",
      "2016-05-05T18:29:41.806915: step 443, loss 0.0714629, acc 0.96875\n",
      "2016-05-05T18:29:42.048405: step 444, loss 0.0533274, acc 0.96875\n",
      "2016-05-05T18:29:42.294468: step 445, loss 0.0289547, acc 1\n",
      "2016-05-05T18:29:42.535002: step 446, loss 0.0884422, acc 0.953125\n",
      "2016-05-05T18:29:42.800170: step 447, loss 0.0890509, acc 0.9375\n",
      "2016-05-05T18:29:42.994146: step 448, loss 0.0269219, acc 1\n",
      "2016-05-05T18:29:43.259868: step 449, loss 0.0467626, acc 0.984375\n",
      "2016-05-05T18:29:43.511969: step 450, loss 0.0723283, acc 0.96875\n",
      "2016-05-05T18:29:43.764051: step 451, loss 0.0576597, acc 0.96875\n",
      "2016-05-05T18:29:44.018964: step 452, loss 0.053052, acc 0.984375\n",
      "2016-05-05T18:29:44.259554: step 453, loss 0.0738069, acc 0.984375\n",
      "2016-05-05T18:29:44.520984: step 454, loss 0.0786163, acc 0.96875\n",
      "2016-05-05T18:29:44.765037: step 455, loss 0.0192227, acc 1\n",
      "2016-05-05T18:29:45.015830: step 456, loss 0.057372, acc 0.984375\n",
      "2016-05-05T18:29:45.267771: step 457, loss 0.0397713, acc 0.984375\n",
      "2016-05-05T18:29:45.515897: step 458, loss 0.0713868, acc 0.96875\n",
      "2016-05-05T18:29:45.769212: step 459, loss 0.0184866, acc 1\n",
      "2016-05-05T18:29:46.013907: step 460, loss 0.114104, acc 0.9375\n",
      "2016-05-05T18:29:46.266877: step 461, loss 0.033878, acc 0.984375\n",
      "2016-05-05T18:29:46.528040: step 462, loss 0.0191541, acc 1\n",
      "2016-05-05T18:29:46.782825: step 463, loss 0.128771, acc 0.96875\n",
      "2016-05-05T18:29:46.981991: step 464, loss 0.166657, acc 0.95\n",
      "2016-05-05T18:29:47.223407: step 465, loss 0.0469887, acc 0.96875\n",
      "2016-05-05T18:29:47.481241: step 466, loss 0.0495273, acc 0.984375\n",
      "2016-05-05T18:29:47.723392: step 467, loss 0.136246, acc 0.96875\n",
      "2016-05-05T18:29:47.957392: step 468, loss 0.0277191, acc 1\n",
      "2016-05-05T18:29:48.206504: step 469, loss 0.127605, acc 0.953125\n",
      "2016-05-05T18:29:48.464893: step 470, loss 0.0730128, acc 0.96875\n",
      "2016-05-05T18:29:48.707975: step 471, loss 0.125269, acc 0.96875\n",
      "2016-05-05T18:29:48.956901: step 472, loss 0.0942597, acc 0.96875\n",
      "2016-05-05T18:29:49.199016: step 473, loss 0.062663, acc 0.96875\n",
      "2016-05-05T18:29:49.458409: step 474, loss 0.0196682, acc 1\n",
      "2016-05-05T18:29:49.724024: step 475, loss 0.0270187, acc 1\n",
      "2016-05-05T18:29:49.974622: step 476, loss 0.102373, acc 0.984375\n",
      "2016-05-05T18:29:50.224561: step 477, loss 0.0514559, acc 0.96875\n",
      "2016-05-05T18:29:50.473418: step 478, loss 0.0741776, acc 0.984375\n",
      "2016-05-05T18:29:50.727943: step 479, loss 0.128861, acc 0.96875\n",
      "2016-05-05T18:29:50.919411: step 480, loss 0.0641846, acc 1\n",
      "2016-05-05T18:29:51.170812: step 481, loss 0.0464378, acc 0.984375\n",
      "2016-05-05T18:29:51.420658: step 482, loss 0.09191, acc 0.96875\n",
      "2016-05-05T18:29:51.663647: step 483, loss 0.0880366, acc 0.96875\n",
      "2016-05-05T18:29:51.918501: step 484, loss 0.122191, acc 0.96875\n",
      "2016-05-05T18:29:52.173953: step 485, loss 0.0166719, acc 1\n",
      "2016-05-05T18:29:52.431533: step 486, loss 0.128374, acc 0.96875\n",
      "2016-05-05T18:29:52.676062: step 487, loss 0.10219, acc 0.953125\n",
      "2016-05-05T18:29:52.940397: step 488, loss 0.0156594, acc 1\n",
      "2016-05-05T18:29:53.191271: step 489, loss 0.115704, acc 0.921875\n",
      "2016-05-05T18:29:53.441266: step 490, loss 0.0223883, acc 1\n",
      "2016-05-05T18:29:53.704271: step 491, loss 0.0410967, acc 0.984375\n",
      "2016-05-05T18:29:53.944272: step 492, loss 0.104342, acc 0.9375\n",
      "2016-05-05T18:29:54.192971: step 493, loss 0.0232131, acc 1\n",
      "2016-05-05T18:29:54.444521: step 494, loss 0.125518, acc 0.96875\n",
      "2016-05-05T18:29:54.687751: step 495, loss 0.0877756, acc 0.984375\n",
      "2016-05-05T18:29:54.873866: step 496, loss 0.0767612, acc 0.95\n",
      "2016-05-05T18:29:55.119459: step 497, loss 0.0623322, acc 0.96875\n",
      "2016-05-05T18:29:55.372100: step 498, loss 0.0494742, acc 0.984375\n",
      "2016-05-05T18:29:55.628339: step 499, loss 0.106048, acc 0.96875\n",
      "2016-05-05T18:29:55.981068: step 500, loss 0.0529392, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2016-05-05T18:29:56.135670: step 500, loss 0.84841, acc 0.59\n",
      "\n",
      "Saved model checkpoint to /Users/charles/Documents/tensorflow/tensorflow/examples/udacity/runs/1462444065/checkpoints/model-500\n",
      "\n",
      "2016-05-05T18:29:56.889447: step 501, loss 0.0188239, acc 1\n",
      "2016-05-05T18:29:57.146416: step 502, loss 0.0664629, acc 0.953125\n",
      "2016-05-05T18:29:57.395526: step 503, loss 0.0512003, acc 0.984375\n",
      "2016-05-05T18:29:57.649176: step 504, loss 0.0294781, acc 0.984375\n",
      "2016-05-05T18:29:57.912373: step 505, loss 0.077113, acc 0.953125\n",
      "2016-05-05T18:29:58.183234: step 506, loss 0.210579, acc 0.9375\n",
      "2016-05-05T18:29:58.449009: step 507, loss 0.0186582, acc 1\n",
      "2016-05-05T18:29:58.692083: step 508, loss 0.0443182, acc 0.984375"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocabulary),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data. Load your own data here\n",
    "print(\"Loading data...\")\n",
    "x_test, y_test, vocabulary, vocabulary_inv = load_data()\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(\"Vocabulary size: {:d}\".format(len(vocabulary)))\n",
    "print(\"Test set size {:d}\".format(len(y_test)))\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(x_test, FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy\n",
    "correct_predictions = float(sum(all_predictions == y_test))\n",
    "print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
